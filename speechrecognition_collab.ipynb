{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "speechrecognition collab",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPBuTRCqvL0bDAWwzLF/00E",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Hanane72/speech_recognition/blob/main/speechrecognition_collab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchaudio\n",
        "import torch.nn as nn\n",
        "import pandas as pd \n",
        "import numpy as np\n",
        "class LogMelSpec(nn.Module):\n",
        "  def  _init_(self, sample_rate=8000, n_mels=128, win_length=160, hop_length=80):\n",
        "    super(LogMelSpec,self)._init_()\n",
        "    self.transform=torchaudio.transforms.MelSpectrogram(\n",
        "        sample_rate=sample_rate, n_mels=n_mels,\n",
        "        win_length=win_length, hop_length=hop_length\n",
        "    )\n",
        "    def forward(self, x):\n",
        "      x=self.transform(x)\n",
        "      x=np.log(x + 1e-14)\n",
        "      return(x)\n",
        "class TextProcess:\n",
        "  def _init_(self):\n",
        "    char_map_str = \"\"\"\n",
        "    ' 0\n",
        "    <SPACE> 1\n",
        "    a 2\n",
        "    b 3\n",
        "    c 4\n",
        "    d 5\n",
        "    e 6\n",
        "    f 7\n",
        "    g 8\n",
        "    h 9\n",
        "    i 10\n",
        "    j 11\n",
        "    k 12\n",
        "    l 13\n",
        "    m 14\n",
        "    n 15\n",
        "    o 16\n",
        "    p 17\n",
        "    q 18\n",
        "    r 19\n",
        "    s 20\n",
        "    t 21\n",
        "    u 22\n",
        "    v 23\n",
        "    w 24\n",
        "    x 25\n",
        "    y 26\n",
        "    z 27\n",
        "    \"\"\"\n",
        "    self.char_map = {} \n",
        "    self.index_map = {} \n",
        "    for line in char_map_str.strip().split(\"\\n\"):\n",
        "      ch, index = line.split()\n",
        "      self.char_map_str[ch] = int(index)\n",
        "      self.index_map[int(index)] = ch\n",
        "      self.index_map[1] = ''\n",
        "  def text_to_int_sequence(self, text):\n",
        "    int_sequence = []\n",
        "    for c in text:\n",
        "      if c == ' ':\n",
        "        ch = self.char_map['<SPACE>'] \n",
        "      else:\n",
        "        ch = self.char_map[c]\n",
        "        int_sequence.append(ch)\n",
        "    return int_sequence\n",
        "  def int_to_text_sequence(self, labels):\n",
        "      string = []\n",
        "      for i in labels:\n",
        "          string.append(self.index_map[1])\n",
        "      return ''.join(string).replace('<SPACE>', ' ')\n",
        "class SpecAugment(nn.Module):\n",
        "  def _init_(self, rate, policy, freq_maks=15, time_mask=35):\n",
        "    super(SpecAugment, self)._init_()\n",
        "\n",
        "    self.rate = rate\n",
        "\n",
        "    self.specaug = nn.Sequential(\n",
        "        torchaudio.transforms.FrequencyMasking(freq_mask_param=freq_mask),\n",
        "        torchaudio.transforms.TimeMasking(time_mask_param=time_mask)\n",
        "    )\n",
        "  def forward(self, x):\n",
        "    return self.specaug(x)\n",
        "\n",
        "class ActDropNormCNN1D(nn.Module):\n",
        "  def _init_(self, n_Feats, dropout, keep_shape=False):\n",
        "    super(ActDropNormCNN1D, self)._init_()\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "    self.norm = nn.LayerNorm(n_feats)\n",
        "    self.keep_shape = keep_shape\n",
        "\n",
        "  def forward(self, x):\n",
        "    x=x.transpose(1, 2)\n",
        "    x=self.dropout(F.gelu(self.norm(x)))\n",
        "    if self.keep.shape:\n",
        "      return x.transpose(1, 2)\n",
        "    else:\n",
        "      return x\n",
        "\n",
        "class SpeechRecognition(nn.Module):\n",
        "  hyper_parameters = {\n",
        "      \"num_classes\": 29,\n",
        "      \"n_feats\": 81,\n",
        "      \"dropout\": 0.1,\n",
        "      \"hidden_size\": 1024,\n",
        "      \"num_layers\": 1\n",
        "  }\n",
        "  def _init_(self, hidden_size, num_classes, n_feats, num_layers, dropout):\n",
        "    super(SpeechRecognition, self)._init_()\n",
        "    self.num_layers = num_layers\n",
        "    self.hidden_size = hidden_size\n",
        "    self.cnn = nn.Sequential(\n",
        "        nn.Convld(n_feats, n_feats, 10, 2, padding=10//2),\n",
        "        ActDropNormCNN1D(n_feats, dropout)\n",
        "    )\n",
        "    self.dense = nn.Sequential(\n",
        "        nn.linear(n_feats, 128),\n",
        "        nn.LayerNorm(128),\n",
        "        nn.GELU(),\n",
        "        nn.Dropout(dropout),\n",
        "        nn.Linear(128, 128),\n",
        "        nn.LayerNorm(128),\n",
        "        nn.GELU(),\n",
        "        nn.Dropout(dropout)\n",
        "    )\n",
        "    self.lstm = nn.LSTM(input_size=128, hidden_size=hidden_size,\n",
        "                        num_layers=num_layers, dropout=0.0,\n",
        "                        bidirectional=False)\n",
        "    self.layer_norm2 = nn.LayerNorm(hidden_size)\n",
        "    self.dropout2 = nn.Dropout(dropout)\n",
        "    self.final_fc=nn.Linear(hidden_size, num_classes)\n",
        "  def _init_hidden(self, batch_size):\n",
        "    n, hs = self.num_layers, self.hidden_size\n",
        "    return (torch.zeros(n+1, batch_size, hs),\n",
        "            torch.zeros(n+1, batch_size, hs))\n",
        "  def forward(self, x, hidden):\n",
        "    x = x.squeeze(1)\n",
        "    x = self.cnn(x)\n",
        "    x = self.dense(x)\n",
        "    x = x.transpose(0, 1)\n",
        "    out, (hn, cn) = self.lstm(x, hidden)\n",
        "    x = self.dropout2(F.gelu(self.layer_norm2(out)))\n",
        "    return self.final_fc(x), (hn, cn)\n",
        "\n",
        "  def train(args):\n",
        "    h_prams = SpeechRecognition.hyper_parameters\n",
        "    h_arams.update(args.hprams_override)\n",
        "\n",
        "    model = SpeechRecognition(**hprams)\n",
        "\n",
        "    logger = TensorboardLogger(args.logdir, name='speech_recognition')\n",
        "    trainer = Trainer(Logger=Logger)\n",
        "\n",
        "    trainer = Trainer(\n",
        "        max_epochs=args.epochs, gpus=arg.gpus,\n",
        "        num_nodes=args.nodes, distributed_backed='ddp',\n",
        "        logger=logger, gradient_clip_val=1.0,\n",
        "        val_check_interval=args.valid_every,\n",
        "        checkpoint_callback=checkpoint_callback(args),\n",
        "        resume_from_checkpoint=args.resum_from_checkpoint\n",
        "    ) \n",
        "    "
      ],
      "metadata": {
        "id": "mvOaLx1FeHfM"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "quyPqEk63F4C"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}